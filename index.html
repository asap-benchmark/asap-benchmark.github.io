
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>ASAP</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                Building Scalable Video Understanding Benchmarks <br> through Sports<br>
                <small>
                    AAAI 2023
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">

                    <li>
                        <a href="https://aniket-agarwal1999.github.io/">
                        Aniket Agarwal*^
                        </a>
                        <br>IIT Roorkee
                    </li>
                    <li>
                        <a href="https://zhalex.com/home">
                        Alex Zhang*
                        </a>
                        <br>Princeton University
                    </li>
                    <li>
                        <a href="https://vishvakmurahari.com/">
                        Vishvak Murahari
                        </a>
                        <br>Princeton University
                    </li>
                    <li>
                        <a href="https://yashkant.github.io/">
                        Yash Kant
                        </a>
                        <br>University of Toronto
                    </li><br>
                    <li>
                        <a href="https://www.gilitschenski.org/igor/">
                        Igor Gilitschenski
                        </a>
                        <br>University of Toronto
                    </li>
                    <li>
                        <a href="https://www.cs.princeton.edu/~karthikn/">
                        Karthik Narasimhan
                        </a>
                        <br>Princeton University
                    </li>
                    <!-- <li>
                        <a href="https://scholar.google.com/citations?user=hBlcEY0AAAAJ">
                          Sherif Abdelkarim*^
                        </a>
                        </br>KAUST
                    </li>
                    <li>
                        <a href="https://aniket-agarwal1999.github.io/">
                            Aniket Agarwal*^
                        </a>
                        </br>IIT Roorkee, KAUST
                    </li>
                    <li>
                        <a href="https://ai.stanford.edu/~optas/">
                          Panos Achlioptas
                        </a>
                        </br>Stanford
                    </li>
                    <li>
                        <a href="https://scholar.google.com/citations?user=9G2OQmkAAAAJ&hl=en">
                          Jun Chen
                        </a>
                        </br>KAUST
                    </li><br>
                    <li>
                        <a href="https://jiaji-huang.github.io/">
                          Jiaji Huang
                        </a>
                        </br>Baidu
                    </li>
                    <li>
                        <a href="http://www.boyangli.org/">
                          Boyang Li
                        </a>
                        </br>NTU Singapore
                    </li>
                    <li>
                        <a href="http://research.baidu.com/People/index-view?id=115">
                          Kenneth Church
                        </a>
                        </br>Baidu
                    </li>
                    <li>
                        <a href="http://www.mohamed-elhoseiny.com/">
                          Mohamed Elhoseiny
                        </a>
                        </br>KAUST
                    </li> -->
                </ul>
                <p> *Denotes Equal Contribution </p>
                <p> ^Work done while working at Princeton University/University of Toronto</p>
            </div>
        </div>


        <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="docs/lcric.pdf">
                            <image src="img/ltvrr_paper.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="docs/lcric.pdf">
                            <image src="img/supp_img.png" height="60px">
                                <h4><strong>Supplementary</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://www.youtube.com/watch?v=dQw4w9WgXcQ">
                            <image src="img/youtube_icon.png" height="60px">
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/Vision-CAIR/LTVRR">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code & Benchmarks</strong></h4>
                            </a>
                        </li>
                        <!-- <li>
                            <a href="https://ltvrr.github.io/challenge/">
                            <image src="img/codalab.png" height="60px">
                                <h4><strong>CodaLab Challenge</strong></h4>
                            </a>
                        </li> -->
                        <!-- <li>
                            <a href="https://github.com/">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li> -->
                    </ul>
                </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    tl;dr
                </h3>
                <image src="img/cricket-intro.png" class="img-responsive" alt="overview"><br>
                <p class="text-justify">
                    Existing benchmarks for evaluating long video understanding falls short on multiple aspects, either lacking in scale or quality of 
                    annotations. These limitations arise from the difficulty in collecting dense annotations for long videos (e.g. actions, dialogues, 
                    etc.), which are often obtained by manually labeling many frames per second. In this work, we introduce an automated <b>A</b>nnotation and 
                    Video <b>S</b>tream <b>A</b>lignment <b>P</b>ipeline (abbreviated ASAP).
                    <br>
                    We then leverage ASAP’s scalability to create <b>LCric</b>, a large-scale long video understanding benchmark, with over 1000 hours of densely 
                    annotated long Cricket videos (with an average sample length of ∼50 mins) collected at virtually <b>zero annotation cost</b>. 
                    
                </p>
            </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    ASAP
                </h3>
                <div class="text-center">
                    <div style="text-align:center;">
                        <!-- <iframe src="https://www.youtube.com/embed/ceEuCXr8Ow8" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe> -->
                        <image src="img/asap-pipeline.gif" class="img-responsive" alt="scales"></image>
                    </div>
                </div>
                <p class="text-justify">
                    We propose an automated annotation pipeline, dubbed as ASAP, where we specifically use sports as video stream data. We specifically use sports because of two reasons, namely:
                    <ol>
                        <li>Presence of varied actions.</li>
                        <li>Presence of rich natural language online commentary.</li>
                    </ol>
                </p>
            </div>
        </div>
        <br>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    LCric
                </h3>
                <p class="text-justify">
                    We use <b>ASAP</b> to create an LVU Benchmark on top of sports of <a href="https://www.youtube.com/watch?v=g-beFHld19c">Cricket</a>, namely <b>LCric</b>. <br>
                    For creating the benchmark we consider 12 atomic events in cricket for forming our query set:
                    <ul>
                        <li><b>Number of runs(points)</b> scored by a team. The number of runs can range from <i>0 to 9</i></li>
                        <li>A foul ball (termed as <b>wide</b>)</li>
                        <li>An out ball (termed as <b>wicket</b>)</li>
                    </ul>
                </p>
                <p style="text-align:center;">
                    <image src="img/lcric-example.png" height="50px" class="img-responsive">
                </p>
                <p class="text-justify">
                    To evaluate on the dataset, we also create automated compositional queries that can test both detection and aggregation skills of a model across a long horizon task.<br>
                    We automatically compose <b>binary</b>, <b>multi-choice</b> and <b>regression</b> queries for the same. Examples of the binary-choice queries can also be
                    seen from the above figure. An algorithm for composing the same can also be found in the supp. section.
                </p>

                <p class="text-justify">
                    We evaluate two of the recent baselines, namely <a href="https://www.robots.ox.ac.uk/~vgg/research/tqn/">TQN</a> and <a href="https://arxiv.org/abs/2201.08383">MemVit</a>, 
                    using our query set. We also use AMT turk study to get a human baseline for our dataset, which clearly suggests a huge scope of improvement for the currently LVU models on
                    LCric Benchmark.
                </p>
                <p style="text-align:center;">
                    <image src="img/lcric-performance.png" height="50px" class="img-responsive">
                </p>
            </div>
        </div>
            


        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    VilHub & RelMix
                </h3>
                <p class="text-justify">
                    To improve upon the classification accuracy on the long-tail spectrum of relationships and objects, we also propose two novel techniques, namely VilHub and RelMix.
                    The main idea behind both of them being able to be used on top of <i>any</i> VRD models so far and improve its overall accuracy, especially on tail classes.
                </p>
                <p style="text-align:center;">
                    <image src="img/ltvrr_approach.png" class="img-responsive" alt="scales">
                </p>
                <p class="text-justify">
                    The overall approach and pipeline used for our baseline models.
                    <br>
                    <br>    
                </p>
                <p class="text-justify">
                    VilHub loss takes its inspiration from the problem of hubness, often talked in NLP problems. This is usually caused when some frequent words, called hubs, gets indistinguishably
                    close to many other less represented words. In long-tail VRR context, these hubs are the head classes, which are often over-predicted at the expense of tail classes. 
                    To alleviate the hubness phenomenon, we develop a vision & language hublessloss (dubbed VilHub).
                </p>
                <p class="text-justify">
                    RelMix is an augmentation technique, inspired from <a href="https://arxiv.org/abs/1710.09412">Mixup</a> and helps alleviate the problem of long-tail by augmenting more tail features
                    in the training set. This is done in a mixup fashion, where triplets are selected belonging to different spectrum of data (i.e., head, med, tail) are combined in a systematic fashion
                    to allow the augmented data having much more of tail features. This in turn helps for long-tail classification and for our overall problem.
                </p>
                <p class="text-justify">
                    Some of the qualitative results using our technique can be viewed below (click on the image to better view). For more qualitative as well quantitative results, please refer to the main paper and supplementary.
                </p>
                <p style="text-align:center;">
                    <a href="img/qualitative_result.jpg"><image src="img/qualitative_result.jpg" class="img-responsive" alt="scales" class="hoverZoomLink"></a>
                </p>
            </div>
        </div> -->
        
            
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <!-- <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@inproceedings{ltvrr2021,
    title={Exploring Long Tail Visual Relationship Recognition with Large Vocabulary},
    author={Abdelkarim, Sherif and Agarwal, Aniket and Achlioptas, Panos and Chen, Jun and Huang, Jiaji and Li, Boyang and Church, Kenneth and Elhoseiny, Mohamed},
    booktitle={Proceedings of the IEEE International Conference on Computer Vision},
    year={2021}
    }
}</textarea>
                </div> -->
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <p>
                The website template was borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a>.
                </p>
            </div>
        </div>

    </div>
</body>
</html>
