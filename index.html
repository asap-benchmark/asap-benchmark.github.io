<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>ASAP</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

    <!--FACEBOOK-->


    <!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
    <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>

    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                Building Scalable Video Understanding Benchmarks <br> through Sports<br>
                <small>
                    <!-- AAAI 2023 -->
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">

                    <li>
                        <a href="https://aniket-agarwal1999.github.io/">
                            Aniket Agarwal*
                        </a>
                        <br>IIT Roorkee
                    </li>
                    <li>
                        <a href="https://zhalex.com/home">
                            Alex Zhang*
                        </a>
                        <br>Princeton University
                    </li>
                    <li>
                        <a href="https://www.cs.princeton.edu/~karthikn/">
                            Karthik Narasimhan
                        </a>
                        <br>Princeton University
                    </li>
                    <li>
                        <a href="https://www.gilitschenski.org/igor/">
                            Igor Gilitschenski
                        </a>
                        <br>University of Toronto
                    </li>
                    <br>
                    <li>
                        <a href="https://vishvakmurahari.com/">
                            Vishvak Murahari^
                        </a>
                        <br>Princeton University
                    </li>
                    <li>
                        <a href="https://yashkant.github.io/">
                            Yash Kant^
                        </a>
                        <br>University of Toronto
                    </li><br>


                    <!-- <li>
                        <a href="https://scholar.google.com/citations?user=hBlcEY0AAAAJ">
                          Sherif Abdelkarim*^
                        </a>
                        </br>KAUST
                    </li>
                    <li>
                        <a href="https://aniket-agarwal1999.github.io/">
                            Aniket Agarwal*^
                        </a>
                        </br>IIT Roorkee, KAUST
                    </li>
                    <li>
                        <a href="https://ai.stanford.edu/~optas/">
                          Panos Achlioptas
                        </a>
                        </br>Stanford
                    </li>
                    <li>
                        <a href="https://scholar.google.com/citations?user=9G2OQmkAAAAJ&hl=en">
                          Jun Chen
                        </a>
                        </br>KAUST
                    </li><br>
                    <li>
                        <a href="https://jiaji-huang.github.io/">
                          Jiaji Huang
                        </a>
                        </br>Baidu
                    </li>
                    <li>
                        <a href="http://www.boyangli.org/">
                          Boyang Li
                        </a>
                        </br>NTU Singapore
                    </li>
                    <li>
                        <a href="http://research.baidu.com/People/index-view?id=115">
                          Kenneth Church
                        </a>
                        </br>Baidu
                    </li>
                    <li>
                        <a href="http://www.mohamed-elhoseiny.com/">
                          Mohamed Elhoseiny
                        </a>
                        </br>KAUST
                    </li> -->
                </ul>
                <p> *Denotes Equal Contribution </p>
                <p> ^Equal Advising</p>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="https://arxiv.org/abs/2301.06866">
                            <image src="img/ltvrr_paper.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="https://arxiv.org/abs/2301.06866">
                            <image src="img/supp_img.png" height="60px">
                                <h4><strong>Supplementary</strong></h4>
                        </a>
                    </li>
                    <!-- <li>
                        <a href="">
                            <image src="img/youtube_icon.png" height="60px">
                                <h4><strong>Video</strong></h4>
                        </a>
                    </li> -->
                    <li>
                        <a href="https://github.com/asap-benchmark/asap-pipeline">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code & Benchmarks</strong></h4>
                        </a>
                    </li>
                    <!-- <li>
                            <a href="https://ltvrr.github.io/challenge/">
                            <image src="img/codalab.png" height="60px">
                                <h4><strong>CodaLab Challenge</strong></h4>
                            </a>
                        </li> -->
                    <!-- <li>
                            <a href="https://github.com/">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li> -->
                </ul>
            </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    tl;dr
                </h3>
                <image src="img/asap.gif" class="img-responsive" alt="overview"><br>
                    <image src="img/text.png" class="img-responsive" alt="overview"><br>
                        <!-- <p class="text-justify">
                        Existing benchmarks for evaluating long video understanding falls short on multiple aspects,
                        either lacking in scale or quality of
                        annotations. These limitations arise from the difficulty in collecting dense annotations for
                        long videos (e.g. actions, dialogues,
                        etc.), which are often obtained by manually labeling many frames per second. In this work, we
                        introduce an automated <b>A</b>nnotation and
                        Video <b>S</b>tream <b>A</b>lignment <b>P</b>ipeline (abbreviated ASAP).
                        <br>
                        We then leverage ASAP’s scalability to create <b>LCric</b>, a large-scale long video
                        understanding benchmark, with over 1000 hours of densely
                        annotated long Cricket videos (with an average sample length of ∼50 mins) collected at virtually
                        <b>zero annotation cost</b>.

                    </p> -->
            </div>
        </div>

        <hr>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    ASAP
                </h3>
                <p class="text-justify">
                    We propose an automated <b>A</b>nnotation and Video <b>S</b>tream <b>A</b>lignment <b>P</b>ipeline,
                    that we dub as <b>ASAP</b>, which aligns video footage of sports
                    matches to online commentary and scorecard information.
                    We specifically target sports because:
                <ol>
                    <li>Video understanding over sports is much easier to define than arbitrary videos.</li>
                    <li>Sports have a wide variety of possible events and actions that occur frequently. </li>
                    <li>There is an abundance of rich natural language online commentary.</li>
                </ol>
                The full annotation pipeline is summarized below:
                </p>
                <div class="text-center">
                    <div style="text-align:center;">
                        <!-- <iframe src="https://www.youtube.com/embed/ceEuCXr8Ow8" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe> -->
                        <image src="img/asap_1.png" class="img-responsive" alt="scales"></image>
                        <image src="img/asap_2.png" class="img-responsive" alt="scales"></image>
                        <hr>
                        <image src="img/asap_3.png" class="img-responsive" alt="scales"></image>
                        <hr>
                    </div>
                </div>
            </div>
        </div>
        <br>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    LCric
                </h3>
                <p class="text-justify">
                    We use <b>ASAP</b> to construct a benchmark for Long-horizon video understanding by annotating
                    matches from the sport of <a href="https://www.youtube.com/watch?v=g-beFHld19c">Cricket</a>, which
                    we call <b>LCric</b>. <br> <br>
                    We chose Cricket because it has a relatively small action space that makes reasoning over it simpler
                    than most long-video understanding problems. This makes it feasible as a benchmark for video
                    understanding models, which currently struggle to handle reasoning over longer time spans.
                    Specifically, our dataset builds on top of twelve distinct events that happen in Cricket:
                <ul>
                    <li><b>The number of runs(points)</b> scored by a team. The number of runs can range from <i>0 to
                            9</i>
                    </li>
                    <li>A foul ball (termed as <b>wide</b>)</li>
                    <li>An out ball (termed as <b>wicket</b>)</li>
                </ul>
                These "atomic" events are then aggregated over a long time span, so we can ask queries to a model about
                what happened over the course of the time span. We call this set of queries our "Query Set".
                </p>
                <p style="text-align:center;">
                    <image src="img/lcric-example.png" height="50px" class="img-responsive">
                </p>
                <p class="text-justify">
                    A model with long horizon video understanding capabilities must be able to localize and detect the
                    occurrence of distinct important events, while also aggregating such information to form a
                    conclusion. In our dataset, we also create automated compositional queries that individually test
                    the detection and aggregation skills of a model across a long horizon task. <br> <br>
                    We automatically compose several types of queries to test the model's reasoning abilities.
                    Specifically,
                <ul>
                    <li><b>binary</b> queries are a yes/no query about the occurrence of some set of events. An example
                        of such a query is shown above.
                    </li>
                    <li><b>multi-choice</b> queries expand on the binary queries by asking the model to use context from
                        the video to select an answer from a set of possible answers. </li>
                    <li><b>regression</b> queries ask the model to determine the number of runs that occurred over a
                        given time horizon. </li>
                </ul>
                Details for how query composition is formed can be found in the supplementary sections.
                </p>

                <p class="text-justify">
                    We evaluate two of the recent baselines, namely <a
                        href="https://www.robots.ox.ac.uk/~vgg/research/tqn/">TQN</a> and <a
                        href="https://arxiv.org/abs/2201.08383">MemVit</a>,
                    using on LCric and the compositional query set. We also ran a set of Amazon Mechanical Turk
                    experiments to get a human baseline for our dataset that greatly outperforms the baselines,
                    suggesting a huge room for improvement for state-of-the-art video understanding models on the LCric
                    Benchmark and general long-horizon video understanding problems.
                </p>
                <p style="text-align:center;">
                    <image src="img/lcric-performance.png" height="50px" class="img-responsive">
                </p>
            </div>
        </div>



        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    VilHub & RelMix
                </h3>
                <p class="text-justify">
                    To improve upon the classification accuracy on the long-tail spectrum of relationships and objects, we also propose two novel techniques, namely VilHub and RelMix.
                    The main idea behind both of them being able to be used on top of <i>any</i> VRD models so far and improve its overall accuracy, especially on tail classes.
                </p>
                <p style="text-align:center;">
                    <image src="img/ltvrr_approach.png" class="img-responsive" alt="scales">
                </p>
                <p class="text-justify">
                    The overall approach and pipeline used for our baseline models.
                    <br>
                    <br>    
                </p>
                <p class="text-justify">
                    VilHub loss takes its inspiration from the problem of hubness, often talked in NLP problems. This is usually caused when some frequent words, called hubs, gets indistinguishably
                    close to many other less represented words. In long-tail VRR context, these hubs are the head classes, which are often over-predicted at the expense of tail classes. 
                    To alleviate the hubness phenomenon, we develop a vision & language hublessloss (dubbed VilHub).
                </p>
                <p class="text-justify">
                    RelMix is an augmentation technique, inspired from <a href="https://arxiv.org/abs/1710.09412">Mixup</a> and helps alleviate the problem of long-tail by augmenting more tail features
                    in the training set. This is done in a mixup fashion, where triplets are selected belonging to different spectrum of data (i.e., head, med, tail) are combined in a systematic fashion
                    to allow the augmented data having much more of tail features. This in turn helps for long-tail classification and for our overall problem.
                </p>
                <p class="text-justify">
                    Some of the qualitative results using our technique can be viewed below (click on the image to better view). For more qualitative as well quantitative results, please refer to the main paper and supplementary.
                </p>
                <p style="text-align:center;">
                    <a href="img/qualitative_result.jpg"><image src="img/qualitative_result.jpg" class="img-responsive" alt="scales" class="hoverZoomLink"></a>
                </p>
            </div>
        </div> -->


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <!-- <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@inproceedings{ltvrr2021,
    title={Exploring Long Tail Visual Relationship Recognition with Large Vocabulary},
    author={Abdelkarim, Sherif and Agarwal, Aniket and Achlioptas, Panos and Chen, Jun and Huang, Jiaji and Li, Boyang and Church, Kenneth and Elhoseiny, Mohamed},
    booktitle={Proceedings of the IEEE International Conference on Computer Vision},
    year={2021}
    }
}</textarea>
                </div> -->
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <p>
                    The website template was borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a>.
                </p>
            </div>
        </div>

    </div>
</body>

</html>
